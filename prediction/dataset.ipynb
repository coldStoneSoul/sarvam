{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f27f3b1",
   "metadata": {},
   "source": [
    "### Dataset ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f2134-0dcc-4d09-92a7-0f0af3aa68b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import uuid\n",
    "import math\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker(\"en_IN\")\n",
    "\n",
    "STATES = [\n",
    "    \"Maharashtra\",\"Delhi\",\"Tamil Nadu\",\"Uttar Pradesh\",\"Gujarat\",\n",
    "    \"Karnataka\",\"Rajasthan\",\"West Bengal\",\"Punjab\",\"Haryana\",\n",
    "    \"Telangana\",\"Andhra Pradesh\",\"Chhattisgarh\",\"Odisha\",\"Bihar\",\n",
    "    \"Jharkhand\",\"Assam\",\"Goa\",\"Manipur\",\"Meghalaya\",\"Mizoram\",\n",
    "    \"Nagaland\",\"Sikkim\",\"Tripura\"\n",
    "]\n",
    "\n",
    "DISPUTE_TYPES = [\n",
    "    \"invoice_non_payment\",\n",
    "    \"interest_on_delay\",\n",
    "    \"goods_rejection\",\n",
    "    \"short_payment\",\n",
    "    \"service_non_payment\",\n",
    "    \"others\"\n",
    "]\n",
    "\n",
    "DOCUMENTS = [\n",
    "    \"invoice\",\n",
    "    \"purchase_order\",\n",
    "    \"delivery_challan\",\n",
    "    \"email_correspondence\"\n",
    "]\n",
    "\n",
    "# Simulate state efficiency factor (legal speed index)\n",
    "STATE_EFFICIENCY = {state: random.uniform(0.8, 1.2) for state in STATES}\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def generate_case():\n",
    "\n",
    "    claim = round(random.uniform(50_000, 5_000_000), 2)\n",
    "    delay_days = random.randint(30, 800)\n",
    "    docs = random.sample(DOCUMENTS, k=random.randint(1, 4))\n",
    "    doc_score = round(len(docs) / 4, 2)\n",
    "    dispute_type = random.choice(DISPUTE_TYPES)\n",
    "    jurisdiction = random.choice(STATES)\n",
    "\n",
    "    # Latent opponent cooperation factor\n",
    "    cooperation_factor = random.uniform(-1, 1)\n",
    "\n",
    "    # --- Domain-Driven Scoring ---\n",
    "\n",
    "    score = 0\n",
    "\n",
    "    # Claim effect (small claims settle faster)\n",
    "    if claim < 500_000:\n",
    "        score += 1.5\n",
    "    elif claim < 2_000_000:\n",
    "        score += 0.5\n",
    "    else:\n",
    "        score -= 1\n",
    "\n",
    "    # Delay effect (longer delay increases pressure)\n",
    "    if delay_days > 365:\n",
    "        score += 2\n",
    "    elif delay_days > 180:\n",
    "        score += 1\n",
    "\n",
    "    # Documentation strength\n",
    "    score += doc_score * 2\n",
    "\n",
    "    # Dispute type effect\n",
    "    if dispute_type in [\"invoice_non_payment\", \"service_non_payment\"]:\n",
    "        score += 1.5\n",
    "    elif dispute_type == \"goods_rejection\":\n",
    "        score -= 1\n",
    "\n",
    "    # Jurisdiction efficiency\n",
    "    score *= STATE_EFFICIENCY[jurisdiction]\n",
    "\n",
    "    # Add cooperation randomness\n",
    "    score += cooperation_factor\n",
    "\n",
    "    # Convert score to probability\n",
    "    settlement_probability = sigmoid(score - 2)\n",
    "\n",
    "    is_settlement = 1 if random.random() < settlement_probability else 0\n",
    "\n",
    "    # Determine final outcome\n",
    "    if is_settlement:\n",
    "        outcome = \"settlement\"\n",
    "        settle_min = round(random.uniform(0.65, 0.85), 2)\n",
    "        settle_max = round(random.uniform(settle_min, 0.95), 2)\n",
    "    else:\n",
    "        outcome = random.choices(\n",
    "            [\"award_in_favor\", \"rejected\", \"pending\"],\n",
    "            weights=[0.4, 0.3, 0.3]\n",
    "        )[0]\n",
    "        settle_min = None\n",
    "        settle_max = None\n",
    "\n",
    "    return {\n",
    "        \"case_id\": f\"SYN_{uuid.uuid4().hex[:10]}\",\n",
    "        \"dispute_type\": dispute_type,\n",
    "        \"claim_amount\": claim,\n",
    "        \"delay_days\": delay_days,\n",
    "        \"document_count\": len(docs),\n",
    "        \"document_completeness_score\": doc_score,\n",
    "        \"jurisdiction\": jurisdiction,\n",
    "        \"final_outcome\": outcome,\n",
    "        \"settlement_min_ratio\": settle_min,\n",
    "        \"settlement_max_ratio\": settle_max,\n",
    "        \"is_settlement\": is_settlement\n",
    "    }\n",
    "\n",
    "\n",
    "# Generate dataset\n",
    "N = 20000\n",
    "data = [generate_case() for _ in range(N)]\n",
    "\n",
    "with open(\"msme_synthetic_cases.json\", \"w\") as f:\n",
    "    import json\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(\"✅ Improved dataset generated:\", len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af6fee8",
   "metadata": {},
   "source": [
    "# Traiing cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e2c73-b5e5-46a2-82ec-a5c157736595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load JSON\n",
    "with open(\"msme_synthetic_cases.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Encode categorical fields\n",
    "dispute_encoder = LabelEncoder()\n",
    "state_encoder = LabelEncoder()\n",
    "df[\"dispute_type_enc\"] = dispute_encoder.fit_transform(df[\"dispute_type\"])\n",
    "df[\"jurisdiction_enc\"] = state_encoder.fit_transform(df[\"jurisdiction\"])\n",
    "\n",
    "FEATURES = [\n",
    "    \"claim_amount\",\n",
    "    \"delay_days\",\n",
    "    \"document_count\",\n",
    "    \"document_completeness_score\",\n",
    "    \"dispute_type_enc\",\n",
    "    \"jurisdiction_enc\"\n",
    "]\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df[\"is_settlement\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_prob > 0.3).astype(int)\n",
    "# cross validation here \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_auc = cross_val_score(\n",
    "    model,\n",
    "    X,\n",
    "    y,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\"\n",
    ")\n",
    "\n",
    "print(\"Cross-Validated AUC Scores:\", cv_auc)\n",
    "print(\"Mean CV AUC:\", round(cv_auc.mean(), 3))\n",
    "print(\"Std Dev CV AUC:\", round(cv_auc.std(), 3))\n",
    "\n",
    "joblib.dump(model, \"xgb_model.pkl\")\n",
    "joblib.dump(dispute_encoder, \"dispute_encoder.pkl\")\n",
    "joblib.dump(state_encoder, \"state_encoder.pkl\")\n",
    "print(\"Model saved as xgb_model.pkl\")\n",
    "print(\"ROC-AUC:\", round(roc_auc_score(y_test, y_prob), 3))\n",
    "print(classification_report(y_test, y_pred))\n",
    "from sklearn.metrics import roc_curve\n",
    "import numpy as np\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "\n",
    "# Youden’s J statistic (tpr - fpr)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(\"Optimal Threshold:\", round(optimal_threshold, 3))\n",
    "\n",
    "# Recalculate predictions\n",
    "y_pred_opt = (y_prob > optimal_threshold).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred_opt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e316ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       feature  importance\n",
      "4             dispute_type_enc    0.301686\n",
      "2               document_count    0.180041\n",
      "1                   delay_days    0.163183\n",
      "0                 claim_amount    0.153753\n",
      "3  document_completeness_score    0.124395\n",
      "5             jurisdiction_enc    0.076944\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": FEATURES,\n",
    "    \"importance\": model.feature_importances_\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2c2a2a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# get contribution values\u001b[39;00m\n\u001b[32m      4\u001b[39m contrib = model.get_booster().predict(\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mxgb\u001b[49m.DMatrix(X_test),\n\u001b[32m      6\u001b[39m     pred_contribs=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# contributions for first case\u001b[39;00m\n\u001b[32m     10\u001b[39m contrib_df = pd.DataFrame({\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature\u001b[39m\u001b[33m\"\u001b[39m: FEATURES + [\u001b[33m\"\u001b[39m\u001b[33mbias\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontribution\u001b[39m\u001b[33m\"\u001b[39m: contrib[\u001b[32m0\u001b[39m]\n\u001b[32m     13\u001b[39m })\n",
      "\u001b[31mNameError\u001b[39m: name 'xgb' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# get contribution values\n",
    "contrib = model.get_booster().predict(\n",
    "    xgb.DMatrix(X_test),\n",
    "    pred_contribs=True\n",
    ")\n",
    "\n",
    "# contributions for first case\n",
    "contrib_df = pd.DataFrame({\n",
    "    \"feature\": FEATURES + [\"bias\"],\n",
    "    \"contribution\": contrib[0]\n",
    "})\n",
    "\n",
    "print(contrib_df.sort_values(by=\"contribution\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccaad05-9b56-488a-bae5-ccf7f2eb181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
